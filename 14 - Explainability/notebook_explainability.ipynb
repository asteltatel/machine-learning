{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nUb0Y5S_WTM"
      },
      "source": [
        "#  LIMEÂ [(Ribeiro .al 2016)](https://arxiv.org/abs/1602.04938)\n",
        "\n",
        "## Intuition\n",
        "*Intuitively, an explanation is a local linear approximation of the model's behaviour. While the model may be very complex globally, it is easier to approximate it around the vicinity of a particular instance. While treating the model as a black box, we perturb the instance we want to explain and learn a sparse linear model around it, as an explanation*\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=https://raw.githubusercontent.com/marcotcr/lime/master/doc/images/lime.png width=400>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "72nARGZzDkSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M66ubXUJQbhv"
      },
      "source": [
        "## Algorithm steps\n",
        "The different steps computed by the algorithm are the following :\n",
        "\n",
        "### 1. Creation of a neighbourhood around the instance :\n",
        "- Data samples are generated by applying perturbations around the instance following a normal distribution\n",
        "- A weight is allocated to every sample with regard to its proximity to the instance. This is the crucial step. The instance explanations may differ a lot with regard to the kernel used to compute the weights. 2 variables are at stake here, the kernel function and the kernel width :\n",
        "  - the kernel function $k$ :\n",
        "  $$k(d, k_w) = exp(\\frac{-d^2}{k_w})$$  \n",
        "  where $$d = \\sqrt{\\sum_{i}^{} (y_i - x_i)^2}$$\n",
        "  - the kernel width $k_w$ :\n",
        "$$k_w = 0.75*\\sqrt{n_f}$$\n",
        "with $n_f$ the number of features in the train set.\n",
        "\n",
        "$k$ and $k_w$ are 2 parameters of our LIME function and can be customised.\n",
        "\n",
        "An example of the impact of the kernel width on the instance explanation :\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=https://christophm.github.io/interpretable-ml-book/images/lime-fail-1.jpeg width=500>\n",
        "</p>\n",
        "\n",
        "### 2. Generate the samples labels\n",
        "Make black-box model predictions on the newly generated neighbourhood dataset to generate the associated labels.\n",
        "\n",
        "### 3. Fit a linear model on the samples\n",
        "A linear model is then fitted to this labeled data in order to generate our local linear model which corresponds to our instance explanation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFAnT0nNOIx_"
      },
      "source": [
        "# LIME for text\n",
        "\n",
        "LIME for text data has one major difference with LIME for tabular data : the way the samples are generated and their weights computed. Let's take again the first step of the algorithm, illustrated with a YouTube comments Spam classification model.\n",
        "\n",
        "|| CONTENT      | CLASS |\n",
        "|-----------| ----------- | ----------- |\n",
        "|267| PSY is a good guy      | 0       |\n",
        "|173| For Christmas Song visit my channel! ;)   | 1        |\n",
        "\n",
        "### 1. Creation of a neighbourhoods around the instance :\n",
        "\n",
        "- Data samples are generated by randomly removing some words from the instance text. The neighbourhood dataset is a dataset a binary features, where the value is 1 if the corresponding word is included and 0 if it has been removed.\n",
        "\n",
        "| For |\tChristmas\t| Song |\tvisit |\tmy |\tchannel! |\t;) |\n",
        "| -- | -- | -- | -- | -- | -- | -- |\n",
        "|1|0|1|1|0|0|1|\n",
        "|0|1|1|1|1|0|1|\n",
        "|1|0|0|1|1|1|1|\n",
        "|1|0|1|1|1|1|1|\n",
        "|0|1|1|1|0|0|1|\n",
        "\n",
        "- A weight is allocated to every sample with regard to its proximity to the instance. With LIME for text, the weight is calculated with the same kernel than for tabular data, with a default kernel width of 25 (kernel width can be customised).\n",
        "\n",
        "| For |\tChristmas\t| Song |\tvisit |\tmy |\tchannel! |\t;) | weight |\n",
        "| -- | -- | -- | -- | -- | -- | -- | -- |\n",
        "|1|0|1|1|0|0|1|0.89|\n",
        "|0|1|1|1|1|0|1|0.92|\n",
        "|1|0|0|1|1|1|1|0.92|\n",
        "|1|0|1|1|1|1|1|0.96|\n",
        "|0|1|1|1|0|0|1|0.89|\n",
        "\n",
        "### 2. Generate the samples labels\n",
        "\n",
        "- This second step is very close to the one for tabular data. The class 1 probability is calculated for every sample using the black-box model's predictions.\n",
        "\n",
        "| For |\tChristmas\t| Song |\tvisit |\tmy |\tchannel! |\t;) | weight | prob |\n",
        "| -- | -- | -- | -- | -- | -- | -- | -- | -- |\n",
        "|1|0|1|1|0|0|1|0.89|0.17|\n",
        "|0|1|1|1|1|0|1|0.92|0.17|\n",
        "|1|0|0|1|1|1|1|0.92|0.99|\n",
        "|1|0|1|1|1|1|1|0.96|0.99|\n",
        "|0|1|1|1|0|0|1|0.89|0.17|\n",
        "\n",
        "### 3. Fit a linear model on the samples\n",
        "- This third step remains the same, a linear model is then fitted to this labeled data in order to generate our local linear model which corresponds to our instance explanation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-6pbYBQbDE9"
      },
      "source": [
        "# Now let's practice !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxoRJURjLRlC"
      },
      "source": [
        "## Packages installation & Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8v1uXHEZbNoj"
      },
      "source": [
        "!pip install lime\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from lime.lime_text import LimeTextExplainer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCj9RMkrLtAt"
      },
      "source": [
        "## Data Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdlGAlO97-Ss"
      },
      "source": [
        "Download the train.csv & test.csv datasets from kaggle ([link](https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset?resource=download))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv(\"../data/ag_news/train.csv\")\n",
        "df_test = pd.read_csv(\"../data/ag_news/test.csv\")\n",
        "\n",
        "df_train"
      ],
      "metadata": {
        "id": "F9O6SqomTD57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJ46NfCCLzdz"
      },
      "source": [
        "## Columns name cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gohYEUF1AYpa"
      },
      "source": [
        "df_train.columns = map(str.lower, df_train.columns)\n",
        "df_test.columns = map(str.lower, df_test.columns)\n",
        "\n",
        "df_train = df_train.rename(columns={'class index': 'target'})\n",
        "df_test = df_test.rename(columns={'class index': 'target'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjNEMXGxMlZm"
      },
      "source": [
        "## TF-IDF Vectorizer & Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Az4950_FNh9"
      },
      "source": [
        "#TF-IDF\n",
        "tfidf_vc = TfidfVectorizer(\n",
        "    min_df = 10,\n",
        "    max_features = 100000,\n",
        "    analyzer = \"word\",\n",
        "    ngram_range = (1, 2),\n",
        "    stop_words = 'english',\n",
        "    lowercase = True\n",
        ")\n",
        "\n",
        "# Logistic Regression\n",
        "model = LogisticRegression(C = 0.5, solver = \"sag\")\n",
        "\n",
        "# Pipeline definition\n",
        "pipe = make_pipeline(tfidf_vc, model)\n",
        "\n",
        "# Pipeline training\n",
        "pipe.fit(df_train[\"description\"], df_train.target)\n",
        "\n",
        "# Predictions on test_set\n",
        "test_pred = pipe.predict(df_test[\"description\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nx1AOaegMxBD"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EKdpt5jFCls"
      },
      "source": [
        "print(classification_report(df_test.target, test_pred))\n",
        "print(confusion_matrix(df_test.target, test_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eGjREQaNbuH"
      },
      "source": [
        "## Explicability with LIME"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1C6x8-4HNHH"
      },
      "source": [
        "idx = df_test.index[0]\n",
        "\n",
        "class_names = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
        "explainer = LimeTextExplainer(class_names = class_names)\n",
        "exp = explainer.explain_instance(\n",
        "    df_test[\"description\"][idx],\n",
        "    pipe.predict_proba,\n",
        "    num_features = 10,\n",
        "    top_labels=3\n",
        ")\n",
        "\n",
        "exp.show_in_notebook(text=df_test[\"description\"][idx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_MGNfcJTS1E"
      },
      "source": [
        "# LIME for image\n",
        "\n",
        "LIME algorithm for images works a little differently than for tabular data and text. Indeed, perturbing individual pixels one by one will not really change the prediction because more than one pixel contribute to one class.\n",
        "\n",
        "\n",
        "\n",
        "## Algorithm steps\n",
        "The different steps computed by the algorithm are the following :\n",
        "\n",
        "### 1. Creation of superpixels :\n",
        "The alorithm first requires to generate \"superpixels\" which are composed of contigous pixels that share properties such as texture or color distribution.This step is crucial for the generation of the LIME explanation since perturbation of superpixels is used to identify which of the image areas has been relevant for a specific class decision.\n",
        "\n",
        "LIME uses the quickshift algorithm to produce these superpixels (more details here : https://www.robots.ox.ac.uk/~vedaldi/assets/pubs/vedaldi08quick.pdf)\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=https://www.oreilly.com/content/wp-content/uploads/sites/2/2019/06/figure3-2cea505fe733a4713eeff3b90f696507.jpg width=500>\n",
        "</p>\n",
        "\n",
        "\n",
        "\n",
        "### 2. Generate perturbed instances :\n",
        "Once the superpixels are defined, we can generate a new dataset of perturbed instances by turning off superpixels on the image. The interpretable representation of the image is a binary vector where 1 indicates the original super-pixel and 0 indicates a grayed out super-pixel.\n",
        "\n",
        "### 3. Fit a linear model on the samples\n",
        "\n",
        "We can now fit a linear model on the perturbed instance to a specific class and highlight the superpixels with positive or negative weight towards a specific class.\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=https://www.oreilly.com/content/wp-content/uploads/sites/2/2019/06/figure4-99d9ea184dd35876e0dbae81f6fce038.jpg width=500>\n",
        "</p>\n",
        "\n",
        "\n",
        "# Now let's practice !\n",
        "\n",
        "## Packages installation & Imports\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDsDzZNlTPaO"
      },
      "source": [
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from lime import lime_image\n",
        "from skimage.segmentation import mark_boundaries\n",
        "from keras.applications import inception_v3 as inc_net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0FyQuA7TZ3Q"
      },
      "source": [
        "## Load pre-trained InceptionV3 model and images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WXAJZxoTPfz"
      },
      "source": [
        "# Load model\n",
        "inception_model = InceptionV3(weights='imagenet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvGmMMGLTPi3"
      },
      "source": [
        "# Image processing\n",
        "def transform_img_fn(path_list):\n",
        "    out = []\n",
        "    for img_path in path_list:\n",
        "        img = image.load_img(img_path, target_size=(299, 299))\n",
        "        x = image.img_to_array(img)\n",
        "        x = np.expand_dims(x, axis=0)\n",
        "        x = inc_net.preprocess_input(x)\n",
        "        out.append(x)\n",
        "    return np.vstack(out)\n",
        "\n",
        "images = transform_img_fn([\"/content/drive/MyDrive/Cours Supaero/LIME image/cat_car.jpeg\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xej91BUTPl0"
      },
      "source": [
        "# display the image\n",
        "plt.imshow(images[0] / 2 + 0.5)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bm8ILUEHTipb"
      },
      "source": [
        "## Make some predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c08vwHNETgdk"
      },
      "source": [
        "# decode the results into a list of tuples (class, description, probability)\n",
        "preds = inception_model.predict(images)\n",
        "for x in decode_predictions(preds)[0]:\n",
        "    print(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km8zL3IyTozx"
      },
      "source": [
        "## Explicability with LIME image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qX8tP21mTPtT"
      },
      "source": [
        "# Train lime image explainer\n",
        "explainer = lime_image.LimeImageExplainer()\n",
        "explanation = explainer.explain_instance(images[0].astype('double'), inception_model.predict, top_labels=5, hide_color=0, num_samples=200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXQcQctfTplX"
      },
      "source": [
        "# Plot boundaries\n",
        "selected_label = 2\n",
        "temp, mask = explanation.get_image_and_mask(explanation.top_labels[selected_label], positive_only=True, num_features=5, hide_rest=True)\n",
        "plt.imshow(mark_boundaries(temp / 2 + 0.5, mask))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcKWuh86Tpop"
      },
      "source": [
        "# Plot boundaries on the full image\n",
        "temp, mask = explanation.get_image_and_mask(explanation.top_labels[selected_label], positive_only=True, num_features=5, hide_rest=False)\n",
        "plt.imshow(mark_boundaries(temp / 2 + 0.5, mask))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJUGfMHITpr_"
      },
      "source": [
        "#Select the same class explained on the figures above.\n",
        "ind =  explanation.top_labels[selected_label]\n",
        "\n",
        "#Map each explanation weight to the corresponding superpixel\n",
        "dict_heatmap = dict(explanation.local_exp[ind])\n",
        "heatmap = np.vectorize(dict_heatmap.get)(explanation.segments)\n",
        "\n",
        "#Plot. The visualization makes more sense if a symmetrical colorbar is used.\n",
        "plt.imshow(heatmap, cmap = 'RdBu', vmin  = -heatmap.max(), vmax = heatmap.max())\n",
        "plt.colorbar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wws_aYMSYKJi"
      },
      "source": [
        "# **SH**apley **A**dditive ex**P**lanations ([Lundberg et .al 2017](https://arxiv.org/abs/1905.04610))\n",
        "\n",
        "## From Game Theory\n",
        "\n",
        "\n",
        "* In game theory, the [Shapley value](https://en.wikipedia.org/wiki/Shapley_value) (1953) is a solution concept of fairly distributing both gains and costs to several actors working in coalition.\n",
        "* The Shapley value applies primarily in situations when the contributions of each actor are unequal, but they work in cooperation with each other to obtain the payoff.\n",
        "\n",
        "You first start by identifying each playerâs contribution when they play individually, when 2 play together, and when all 3 play together.\n",
        "<p align=\"center\">\n",
        "<img src=https://clearcode.cc/app/uploads/2016/11/ABC-wide.png width=500>\n",
        "</p>\n",
        "\n",
        "Then, you need to consider all possible orders and calculate their marginal value â e.g. what value does each player add when player A enters the game first, followed by player B, and then player C.\n",
        "Below are the 6 possible orders and the marginal value each player adds in the different combinations:\n",
        "<p align=\"center\">\n",
        "<img src=https://clearcode.cc/app/uploads/2016/11/ABC-updated.png width=500>\n",
        "</p>\n",
        "\n",
        "Now that we have calculated each playerâs marginal value across all 6 possible order combinations, we now need to add them up and work out the Shapley value (i.e. the average) for each player.\n",
        "\n",
        "<ins>Example for Player A:</ins>\n",
        "$ \\text{Shapley}_{value} = \\frac{7+7+10+3+9+10}{6} \\approx 7.7$\n",
        "\n",
        "Computing the Shapley value for each player will give the true contribution each player made to the game and assign credit fairly\n",
        "\n",
        "## To Explainability Method\n",
        "\n",
        "* Each value of an independent variable or a feature for a given sample is a part of a cooperative game where we assume that prediction is actually the payout.\n",
        "* Shapley values correspond to the contribution of each feature towards pushing the prediction away from the expected value.\n",
        "\n",
        "Let take an example of the median house value prediction for California districts\n",
        "<p align=\"center\">\n",
        "<img src=https://raw.githubusercontent.com/shap/shap/master/docs/artwork/california_waterfall.png width=700>\n",
        "</p>\n",
        "\n",
        "Example of features definition:\n",
        "* MedInc: median income in block group\n",
        "* HouseAge: median house age in block group\n",
        "* AveRooms: average number of rooms per household\n",
        "* AveBedrms: average number of bedrooms per household\n",
        "* Population: block group population\n",
        "* AveOccup: average number of household members\n",
        "* Latitude: block group latitude\n",
        "* Longitude: block group longitude\n",
        "\n",
        "For more information, link to [California housing prices dataset](https://inria.github.io/scikit-learn-mooc/python_scripts/datasets_california_housing.html)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbpyrYIeeBtZ"
      },
      "source": [
        "## Explanation of SHAP through visualization\n",
        "\n",
        "### Global explainability & local explanation summary\n",
        "<p align=\"center\">\n",
        "<img src=https://raw.githubusercontent.com/shap/shap/master/docs/artwork/california_global_bar.png width=470>\n",
        "<img src=https://raw.githubusercontent.com/shap/shap/master/docs/artwork/california_beeswarm.png width=530>\n",
        "</p>\n",
        "\n",
        "### Local explainability and correlation\n",
        "<p align=\"center\">\n",
        "<img src=https://raw.githubusercontent.com/shap/shap/master/docs/artwork/california_scatter.png width=500>\n",
        "</p>\n",
        "\n",
        "## Advantages\n",
        "* SHAP has a solid theoretical foundation in game theory. The prediction is fairly distributed among the feature values. We get contrastive explanations that compare the prediction with the average prediction.\n",
        "* SHAP has a fast implementation for tree-based models.\n",
        "* When computation of the many Shapley values is possible, global model interpretations can be built. The global interpretation methods include feature importance, feature dependence, interactions, clustering and summary plots.\n",
        "\n",
        "## Drawbacks\n",
        "* Slow computation if you want to compute Shapley values for many instances (except for tree-based models).\n",
        "* The disadvantages of Shapley values also apply to SHAP: Shapley values can be misinterpreted.\n",
        "* Since every model is trained from observational data, it is not necessarily a causal model.\n",
        "\n",
        "For more information on SHAP values see: https://github.com/shap/shap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvTx8iJFgyJI"
      },
      "source": [
        "## Practical exercise\n",
        "\n",
        "Download the dataset from Kaggle ([link](https://www.kaggle.com/paololol/league-of-legends-ranked-matches))\n",
        "\n",
        "The objective in a game of League of Legends is to destroy the enemy base, in a 5 vs. 5 match. Using datasets with statistics of the game and the players, the goal is to predict the probability to win the game."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBsbQvdldRLb"
      },
      "source": [
        "! pip install shap\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "shap.initjs()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJp0I7THgz61"
      },
      "source": [
        "# read in the data\n",
        "matches = pd.read_csv(\"/content/drive/MyDrive/Cours Supaero/SHAP/matches.csv\")\n",
        "participants = pd.read_csv(\"/content/drive/MyDrive/Cours Supaero/SHAP/participants.csv\")\n",
        "stats1 = pd.read_csv(\"/content/drive/MyDrive/Cours Supaero/SHAP/stats1.csv\", low_memory=False)\n",
        "stats2 = pd.read_csv(\"/content/drive/MyDrive/Cours Supaero/SHAP/stats2.csv\", low_memory=False)\n",
        "stats = pd.concat([stats1,stats2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxMEPd-VrXnf"
      },
      "source": [
        "# merge into a single DataFrame\n",
        "a = pd.merge(participants, matches, left_on=\"matchid\", right_on=\"id\")\n",
        "allstats_orig = pd.merge(a, stats, left_on=\"matchid\", right_on=\"id\")\n",
        "allstats = allstats_orig.copy()\n",
        "\n",
        "# drop games that lasted less than 10 minutes\n",
        "allstats = allstats.loc[allstats[\"duration\"] >= 10*60,:]\n",
        "\n",
        "# Convert string-based categories to numeric values\n",
        "cat_cols = [\"role\", \"position\", \"version\", \"platformid\"]\n",
        "for c in cat_cols:\n",
        "    allstats[c] = allstats[c].astype('category')\n",
        "    allstats[c] = allstats[c].cat.codes\n",
        "allstats[\"wardsbought\"] = allstats[\"wardsbought\"].astype(np.int32)\n",
        "\n",
        "# Reduce dataset size to accelerate training\n",
        "allstats = allstats[allstats['matchid'] < 50000]\n",
        "\n",
        "X = allstats.drop([\"win\"], axis=1)\n",
        "y = allstats[\"win\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usVtcWWJrc31"
      },
      "source": [
        "# convert all features we want to consider as rates\n",
        "rate_features = [\n",
        "    \"kills\", \"deaths\", \"assists\", \"killingsprees\", \"doublekills\",\n",
        "    \"triplekills\", \"quadrakills\", \"pentakills\", \"legendarykills\",\n",
        "    \"totdmgdealt\", \"magicdmgdealt\", \"physicaldmgdealt\", \"truedmgdealt\",\n",
        "    \"totdmgtochamp\", \"magicdmgtochamp\", \"physdmgtochamp\", \"truedmgtochamp\",\n",
        "    \"totheal\", \"totunitshealed\", \"dmgtoobj\", \"timecc\", \"totdmgtaken\",\n",
        "    \"magicdmgtaken\" , \"physdmgtaken\", \"truedmgtaken\", \"goldearned\", \"goldspent\",\n",
        "    \"totminionskilled\", \"neutralminionskilled\", \"ownjunglekills\",\n",
        "    \"enemyjunglekills\", \"totcctimedealt\", \"pinksbought\", \"wardsbought\",\n",
        "    \"wardsplaced\", \"wardskilled\"\n",
        "]\n",
        "for feature_name in rate_features:\n",
        "    X[feature_name] /= X[\"duration\"] / 60 # per minute rate\n",
        "\n",
        "# convert to fraction of game\n",
        "X[\"longesttimespentliving\"] /= X[\"duration\"]\n",
        "\n",
        "# define friendly names for the features\n",
        "full_names = {\n",
        "    \"kills\": \"Kills per min.\",\n",
        "    \"deaths\": \"Deaths per min.\",\n",
        "    \"assists\": \"Assists per min.\",\n",
        "    \"killingsprees\": \"Killing sprees per min.\",\n",
        "    \"longesttimespentliving\": \"Longest time living as % of game\",\n",
        "    \"doublekills\": \"Double kills per min.\",\n",
        "    \"triplekills\": \"Triple kills per min.\",\n",
        "    \"quadrakills\": \"Quadra kills per min.\",\n",
        "    \"pentakills\": \"Penta kills per min.\",\n",
        "    \"legendarykills\": \"Legendary kills per min.\",\n",
        "    \"totdmgdealt\": \"Total damage dealt per min.\",\n",
        "    \"magicdmgdealt\": \"Magic damage dealt per min.\",\n",
        "    \"physicaldmgdealt\": \"Physical damage dealt per min.\",\n",
        "    \"truedmgdealt\": \"True damage dealt per min.\",\n",
        "    \"totdmgtochamp\": \"Total damage to champions per min.\",\n",
        "    \"magicdmgtochamp\": \"Magic damage to champions per min.\",\n",
        "    \"physdmgtochamp\": \"Physical damage to champions per min.\",\n",
        "    \"truedmgtochamp\": \"True damage to champions per min.\",\n",
        "    \"totheal\": \"Total healing per min.\",\n",
        "    \"totunitshealed\": \"Total units healed per min.\",\n",
        "    \"dmgtoobj\": \"Damage to objects per min.\",\n",
        "    \"timecc\": \"Time spent with crown control per min.\",\n",
        "    \"totdmgtaken\": \"Total damage taken per min.\",\n",
        "    \"magicdmgtaken\": \"Magic damage taken per min.\",\n",
        "    \"physdmgtaken\": \"Physical damage taken per min.\",\n",
        "    \"truedmgtaken\": \"True damage taken per min.\",\n",
        "    \"goldearned\": \"Gold earned per min.\",\n",
        "    \"goldspent\": \"Gold spent per min.\",\n",
        "    \"totminionskilled\": \"Total minions killed per min.\",\n",
        "    \"neutralminionskilled\": \"Neutral minions killed per min.\",\n",
        "    \"ownjunglekills\": \"Own jungle kills per min.\",\n",
        "    \"enemyjunglekills\": \"Enemy jungle kills per min.\",\n",
        "    \"totcctimedealt\": \"Total crown control time dealt per min.\",\n",
        "    \"pinksbought\": \"Pink wards bought per min.\",\n",
        "    \"wardsbought\": \"Wards bought per min.\",\n",
        "    \"wardsplaced\": \"Wards placed per min.\",\n",
        "    \"turretkills\": \"# of turret kills\",\n",
        "    \"inhibkills\": \"# of inhibitor kills\",\n",
        "    \"dmgtoturrets\": \"Damage to turrets\"\n",
        "}\n",
        "feature_names = [full_names.get(n, n) for n in X.columns]\n",
        "X.columns = feature_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufCmizXxmOmh"
      },
      "source": [
        "# create train/validation split\n",
        "Xt, Xv, yt, yv = train_test_split(X,y, test_size=0.2, random_state=10)\n",
        "dt = xgb.DMatrix(Xt, label=yt.values)\n",
        "dv = xgb.DMatrix(Xv, label=yv.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ratjSrM_nh3c"
      },
      "source": [
        "# We want to solve a logistic regression with a logloss evaluation\n",
        "params = {\n",
        "    \"eta\": 0.5,\n",
        "    \"max_depth\": 4,\n",
        "    \"objective\": \"binary:logistic\",\n",
        "    \"silent\": 1,\n",
        "    \"base_score\": np.mean(yt),\n",
        "    \"eval_metric\": \"logloss\"\n",
        "}\n",
        "# Code the training part for 200 iterations with early stopping rounds at 5 and a verbose eval at 25\n",
        "model = xgb.train(params, dt, 200, [(dt, \"train\"),(dv, \"valid\")], early_stopping_rounds=5, verbose_eval=25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_J-o4zTHnj3z"
      },
      "source": [
        "# compute the SHAP values for every prediction in the validation dataset\n",
        "explainer = shap.TreeExplainer(model)\n",
        "explainer_Xv = explainer(Xv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap_values = explainer_Xv.values\n",
        "base_values = explainer_Xv.base_values"
      ],
      "metadata": {
        "id": "XnI61pqr_7Aw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exp = shap.Explanation(shap_values, base_values, data=Xv.values, feature_names=Xv.columns)\n",
        "\n",
        "idx=0\n",
        "\n",
        "shap.plots.waterfall(exp[idx])"
      ],
      "metadata": {
        "id": "7ygD3h7Km5qS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejoFle1lnn2W"
      },
      "source": [
        "# Force plot example for a record\n",
        "shap.initjs()\n",
        "shap.force_plot(explainer.expected_value, shap_values[idx,:], Xv.iloc[idx,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgOkFff6noid"
      },
      "source": [
        "xs = np.linspace(-4,4,100)\n",
        "plt.xlabel(\"Log odds of winning\")\n",
        "plt.ylabel(\"Probability of winning\")\n",
        "plt.title(\"How changes in log odds convert to probability of winning\")\n",
        "plt.plot(xs, 1/(1+np.exp(-xs)))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSshOsjou1UH"
      },
      "source": [
        "# Global explainability\n",
        "shap.plots.bar(explainer_Xv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_50_teunroK"
      },
      "source": [
        "# Local explanation summary\n",
        "shap.summary_plot(shap_values, Xv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vJ6WjzVohjr"
      },
      "source": [
        "# Dependence plot between variables (automatic)\n",
        "shap.dependence_plot(\"Gold earned per min.\", shap_values, Xv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhqwIxjent6R"
      },
      "source": [
        "# Dependence plot between variables (assigned)\n",
        "shap.dependence_plot(\"Gold earned per min.\", shap_values, Xv, alpha=0.2, interaction_index=\"Deaths per min.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10fT0tR5nvyS"
      },
      "source": [
        "# sort the features indexes by their importance in the model\n",
        "# (sum of SHAP value magnitudes over the validation dataset)\n",
        "top_inds = np.argsort(-np.sum(np.abs(shap_values), 0))\n",
        "\n",
        "# make SHAP plots of the three most important features\n",
        "for i in range(3):\n",
        "    shap.dependence_plot(top_inds[i], shap_values, Xv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LL0iolJYn0_D"
      },
      "source": [
        "# Play with plot variables\n",
        "shap.dependence_plot(top_inds[9], shap_values, Xv, x_jitter=0.5, alpha=0.2, dot_size=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXkIjXSkqR0L"
      },
      "source": [
        "## Going further with Explainability\n",
        "\n",
        "### SHAPASH ([Github](https://github.com/MAIF/shapash))\n",
        "\n",
        "A module developped by MAIF using SHAP methodology with nice features such as a web app for exploration and ML OPS usage.\n",
        "\n",
        "[Demo](https://shapash-demo.ossbymaif.fr/) of the dashboarding capabilities.\n",
        "\n",
        "[Notebook](https://github.com/MAIF/shapash/blob/master/tutorial/tutorial03-Shapash-overview-model-in-production.ipynb) example for ML OPS usage\n",
        "\n",
        "#### Strengths\n",
        "* A great tool for data scientists to investigate a model's behaviour faster !\n",
        "* Ongoing development today to add new features\n",
        "\n",
        "#### Weaknesses\n",
        "* It \"just\" a plotly layer on top of SHAP not a end-user-driven Framework for model explanations\n",
        "* Decision making based on the graphs is not immediate, it only provides insights\n",
        "* Audiences need to be a bit technical to be confortable with the approach"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtIsw1axqHer"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}