{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\" /></a>&nbsp;| [Emmanuel Rachelson](https://personnel.isae-supaero.fr/emmanuel-rachelson?lang=en) | <a href=\"https://supaerodatascience.github.io/machine-learning/\">https://supaerodatascience.github.io/machine-learning/</a>\n",
    "\n",
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">XGBoost<br>Introduction to XGBoost</div>\n",
    "\n",
    "This Practice Course is composed of 3 parts - each part is meant to be done in about 1 hour :\n",
    "* In the **first notebook**, you learned the **basic of XGBoost**, how to apply it on a dataset and tune it to obtain the best performances.\n",
    "* In the **second notebook**, we focused on **ensemble methods**.\n",
    "* Finally in the **last notebook** you will see how the choice of a method (such as XGBoost) is a key element of a tradeoff between **Bias and Variance**. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by a few reminder on the Bias/Variance tradeoff :\n",
    "\n",
    "\n",
    "**Bias** is the mean error between our prediction and the correct value. A high bias means that our model makes large mistakes, even on the training dataset.\n",
    "\n",
    "**Variance** is the sensitivity of the prediction to small changes in the dataset. A high variance means that the model performs well on the training dataset, but poorly on the test dataset.\n",
    "\n",
    "Let's summarize that with a figure :\n",
    "\n",
    "<img src=\"img/1 xwtSpR_zg7j7zusa4IDHNQ.png\">\n",
    "\n",
    "In this figure, the center of the target is what we try to predict - as if we were trying to send an arrow in the bullseye zone.\n",
    "A model with a high bias cannot reach the center - the center of gravity of all its arrows is on the outer part of the target.\n",
    "A model with a low bias is globally centered on the target.\n",
    "\n",
    "A model with a high variance is not consistant - its arrows are largely spread.\n",
    "A model with low variance is very consistant - even when its arrows are not in the right place, they are in a limited zone.\n",
    "\n",
    "In a machine learning algorithm, it is a *tradeoff* between Bias and Variance : we want to minimize both (low bias, low variance), but usually when one decreases the other increases.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To control overfitting in Gradient Boosted Trees, we can use 3 main leverages : tree structure, shrinkage and randomization.\n",
    "\n",
    "We already saw that the tree parameters, especially those related to tree depth such as max_depth and gamma, is a way to control the complexity of the model, and therefore the tradeoff bias/variance.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>Exercice 1</b><br>\n",
    "      Complete the following code to compare the influence of the number of tree on both the bias and the variance. Complete the annotations.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (999,) and (0,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 38\u001b[0m\n\u001b[1;32m     34\u001b[0m     estim_list\u001b[38;5;241m.\u001b[39mappend(n_estimators)\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m---> 38\u001b[0m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestim_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m         \u001b[49m\u001b[43mlinewidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m ax\u001b[38;5;241m.\u001b[39mplot(estim_list,train_list, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain\u001b[39m\u001b[38;5;124m'\u001b[39m, linewidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     41\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_ylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/mambaforge/envs/py-latest/lib/python3.12/site-packages/matplotlib/axes/_axes.py:1779\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1776\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1777\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1778\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[0;32m-> 1779\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[1;32m   1781\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m~/mambaforge/envs/py-latest/lib/python3.12/site-packages/matplotlib/axes/_base.py:296\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, axes, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    295\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 296\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/py-latest/lib/python3.12/site-packages/matplotlib/axes/_base.py:486\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    483\u001b[0m     axes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 486\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    487\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    490\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (999,) and (0,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def ground_truth(x):\n",
    "    \"\"\"Ground truth -- function to approximate\"\"\"\n",
    "    return x * np.sin(x) + np.sin(2 * x)\n",
    "\n",
    "def gen_data(n_samples=200):\n",
    "    \"\"\"generate training and testing data\"\"\"\n",
    "    np.random.seed(13)\n",
    "    x = np.random.uniform(0, 10, size=n_samples)\n",
    "    x.sort()\n",
    "    y = ground_truth(x) + 0.75 * np.random.normal(size=n_samples)\n",
    "    train_mask = np.random.randint(0, 2, size=n_samples).astype(np.bool)\n",
    "    x_train, y_train = x[train_mask, np.newaxis], y[train_mask]\n",
    "    x_test, y_test = x[~train_mask, np.newaxis], y[~train_mask]\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = gen_data(200)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 5))\n",
    "ax = plt.gca()\n",
    "\n",
    "test_list = []\n",
    "train_list = []\n",
    "estim_list = []\n",
    "\n",
    "for n_estimators in range(1,1000):\n",
    "    estim_list.append(n_estimators)\n",
    "    ...\n",
    "    \n",
    "    \n",
    "ax.plot(estim_list, test_list, label='Test',\n",
    "         linewidth=2)\n",
    "ax.plot(estim_list,train_list, label='Train', linewidth=2)\n",
    "ax.set_ylabel('Error')\n",
    "ax.set_xlabel('n_estimators')\n",
    "ax.set_ylim((0, 2))\n",
    "\n",
    "ax.annotate('... bias', xy=(900, train_list[900]), xycoords='data',\n",
    "            xytext=(600, 0.3), textcoords='data',\n",
    "            arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc\"),\n",
    "            )\n",
    "ax.annotate('... variance', xy=(900, test_list[900]), xycoords='data',\n",
    "            xytext=(600, 0.4), textcoords='data',\n",
    "            arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc\"),\n",
    "            )\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shrinkage\n",
    "\n",
    "One of the most usefull technic for regularization in Gradient Boosted Trees is the Shrinkage : the idea is to slow down the training by reducing the prediction of each tree by a scalar - the learning rate. In this way, the model must produce stronger concepts. \n",
    "\n",
    "A low learning rate impose a greater number of trees (n_estimators) to have a similar error during training - we are trading training time for some more precision.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>Exercice 2</b><br>\n",
    "      Reuse and modify the previous code to compare two differente learning rates : 1.0 and 0.1. Conclude by completing the annotations.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(figsize=(8, 5))\n",
    "ax = plt.gca()\n",
    "\n",
    "test_list = []\n",
    "train_list = []\n",
    "estim_list = []\n",
    "\n",
    "test_list_2 = []\n",
    "train_list_2 = []\n",
    "\n",
    "for n_estimators in range(1,1000):\n",
    "    estim_list.append(n_estimators)\n",
    "    est = ...\n",
    "    \n",
    "    est2 = ...\n",
    "    \n",
    "    \n",
    "ax.plot(estim_list, test_list_2, label='Test 0.1',\n",
    "         linewidth=2)\n",
    "ax.plot(estim_list,train_list_2, label='Train 0.1', linewidth=2)\n",
    "\n",
    "ax.plot(estim_list, test_list, label='Test 1.0',\n",
    "         linewidth=2)\n",
    "ax.plot(estim_list,train_list, label='Train 1.0', linewidth=2)\n",
    "ax.set_ylabel('Error')\n",
    "ax.set_xlabel('n_estimators')\n",
    "ax.set_ylim((0, 2))\n",
    "\n",
    "\n",
    "\n",
    "ax.annotate('Requires ... trees', xy=(200, train_list_2[199]), xycoords='data',\n",
    "            xytext=(300, 1.0), textcoords='data',\n",
    "            arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc\"),\n",
    "            )\n",
    "ax.annotate('... test error', xy=(900, test_list_2[899]), xycoords='data',\n",
    "            xytext=(600, 0.5), textcoords='data',\n",
    "            arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc\"),\n",
    "            )\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Boosting\n",
    "\n",
    "As for many other algorithms, introducing randomness during training can lead to a greater precision at the end. There are two main parameters that introduce randomness :\n",
    "* We can subsample the training data before growing each tree\n",
    "* We can subsample the features before choosing the best split node\n",
    "\n",
    "[XGBoost list of parameters](https://xgboost.readthedocs.io/en/latest/parameter.html)\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>Exercice 3</b><br>\n",
    "      In XGBoost list of parameters, find the one that control randomness during training.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Sources :\n",
    "* https://xgboost.readthedocs.io/en/latest/tutorials/model.html\n",
    "* https://xgboost.readthedocs.io/en/latest/tutorials/param_tuning.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": false,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
